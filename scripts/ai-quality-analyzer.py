#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
WebSys Phase2 AIæ´»ç”¨å“è³ªåˆ†æã‚·ã‚¹ãƒ†ãƒ 
ä½œæˆæ—¥: 2025-09-30
ç›®çš„: AI/GPTçµ±åˆã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªåˆ†æãƒ»æ”¹å–„ææ¡ˆãƒ»è‡ªå‹•è©•ä¾¡
"""

import json
import os
import sys
import datetime
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple
import argparse

class AIQualityAnalyzer:
    def __init__(self, docs_dir: str = "docs", output_dir: str = "docs/quality-reports"):
        self.docs_dir = Path(docs_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.datetime.now()

        # AIåˆ†æã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå°†æ¥çš„ã«ã¯GPT APIçµ±åˆï¼‰
        self.ai_enabled = False  # å®Ÿéš›ã®AI APIãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹

    def analyze_content_quality(self) -> Dict[str, Any]:
        """AIæ´»ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªåˆ†æ"""
        print("ğŸ¤– AIå“è³ªåˆ†æé–‹å§‹...")

        md_files = list(self.docs_dir.glob("**/*.md"))

        analysis_results = {
            "metadata": {
                "timestamp": self.timestamp.isoformat(),
                "analyzer": "AIQualityAnalyzer v1.0",
                "ai_enabled": self.ai_enabled,
                "total_files": len(md_files)
            },
            "content_analysis": {},
            "readability_scores": {},
            "structure_scores": {},
            "improvement_suggestions": {},
            "ai_recommendations": [],
            "quality_summary": {}
        }

        for file in md_files:
            print(f"ğŸ” åˆ†æä¸­: {file.name}")
            file_analysis = self._analyze_single_file(file)
            analysis_results["content_analysis"][str(file)] = file_analysis

        # å…¨ä½“ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        analysis_results["quality_summary"] = self._generate_quality_summary(
            analysis_results["content_analysis"]
        )

        # AIæ¨å¥¨äº‹é …ç”Ÿæˆ
        analysis_results["ai_recommendations"] = self._generate_ai_recommendations(
            analysis_results["content_analysis"]
        )

        return analysis_results

    def _analyze_single_file(self, file_path: Path) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            lines = content.split('\n')
            words = len(content.split())
            chars = len(content)

            # æ§‹é€ åˆ†æ
            headers = self._analyze_headers(lines)
            links = self._analyze_links(content)
            images = self._analyze_images(content)
            code_blocks = self._analyze_code_blocks(content)
            tables = self._analyze_tables(lines)

            # å¯èª­æ€§åˆ†æ
            readability = self._analyze_readability(content, words)

            # æ§‹é€ å“è³ªã‚¹ã‚³ã‚¢
            structure_score = self._calculate_structure_score(
                headers, links, images, code_blocks, tables, words
            )

            # AIå“è³ªåˆ†æï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            ai_analysis = self._simulate_ai_analysis(content, file_path.name)

            return {
                "basic_metrics": {
                    "lines": len(lines),
                    "words": words,
                    "characters": chars,
                    "avg_line_length": chars / len(lines) if lines else 0
                },
                "structure_analysis": {
                    "headers": headers,
                    "links": links,
                    "images": images,
                    "code_blocks": code_blocks,
                    "tables": tables
                },
                "readability": readability,
                "structure_score": structure_score,
                "ai_analysis": ai_analysis,
                "overall_score": self._calculate_overall_score(
                    structure_score, readability["score"], ai_analysis["score"]
                )
            }

        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return {"error": str(e)}

    def _analyze_headers(self, lines: List[str]) -> Dict[str, Any]:
        """è¦‹å‡ºã—æ§‹é€ åˆ†æ"""
        headers = []
        header_hierarchy = []

        for i, line in enumerate(lines):
            line = line.strip()
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('#').strip()
                headers.append({
                    "level": level,
                    "text": text,
                    "line_number": i + 1
                })

        # éšå±¤æ§‹é€ ãƒã‚§ãƒƒã‚¯
        hierarchy_issues = []
        for i in range(1, len(headers)):
            current_level = headers[i]["level"]
            previous_level = headers[i-1]["level"]

            if current_level > previous_level + 1:
                hierarchy_issues.append({
                    "line": headers[i]["line_number"],
                    "issue": f"è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«{current_level}ãŒ{previous_level}ã‹ã‚‰é£›èº"
                })

        return {
            "total": len(headers),
            "hierarchy": headers,
            "hierarchy_issues": hierarchy_issues,
            "deepest_level": max([h["level"] for h in headers]) if headers else 0
        }

    def _analyze_links(self, content: str) -> Dict[str, Any]:
        """ãƒªãƒ³ã‚¯åˆ†æ"""
        # å†…éƒ¨ãƒªãƒ³ã‚¯
        internal_links = re.findall(r'\[([^\]]*)\]\(\./([^)]*)\)', content)

        # å¤–éƒ¨ãƒªãƒ³ã‚¯
        external_links = re.findall(r'\[([^\]]*)\]\((https?://[^)]*)\)', content)

        # ç©ºãƒªãƒ³ã‚¯
        empty_links = re.findall(r'\[\]\([^)]*\)', content)

        return {
            "total": len(internal_links) + len(external_links),
            "internal": len(internal_links),
            "external": len(external_links),
            "empty": len(empty_links),
            "internal_details": internal_links,
            "external_details": external_links
        }

    def _analyze_images(self, content: str) -> Dict[str, Any]:
        """ç”»åƒåˆ†æ"""
        images = re.findall(r'!\[([^\]]*)\]\(([^)]*)\)', content)
        images_without_alt = re.findall(r'!\[\]\([^)]*\)', content)

        return {
            "total": len(images),
            "with_alt": len([img for img in images if img[0]]),
            "without_alt": len(images_without_alt),
            "details": images
        }

    def _analyze_code_blocks(self, content: str) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯åˆ†æ"""
        # ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ3ã¤ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
        code_blocks = re.findall(r'```(\w+)?\n(.*?)\n```', content, re.DOTALL)

        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰
        inline_code = re.findall(r'`([^`]+)`', content)

        languages = [block[0] for block in code_blocks if block[0]]
        language_count = {}
        for lang in languages:
            language_count[lang] = language_count.get(lang, 0) + 1

        return {
            "total_blocks": len(code_blocks),
            "inline_code": len(inline_code),
            "languages": language_count,
            "details": code_blocks
        }

    def _analyze_tables(self, lines: List[str]) -> Dict[str, Any]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«åˆ†æ"""
        table_lines = [line for line in lines if '|' in line and line.strip().startswith('|')]

        # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆé€£ç¶šã™ã‚‹è¡¨å½¢å¼è¡Œã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
        tables = 0
        in_table = False

        for line in lines:
            is_table_line = '|' in line and line.strip().startswith('|')
            if is_table_line and not in_table:
                tables += 1
                in_table = True
            elif not is_table_line:
                in_table = False

        return {
            "total": tables,
            "total_rows": len(table_lines)
        }

    def _analyze_readability(self, content: str, word_count: int) -> Dict[str, Any]:
        """å¯èª­æ€§åˆ†æ"""
        # æ–‡ã®æ•°ï¼ˆãƒ”ãƒªã‚ªãƒ‰ã€æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã§åŒºåˆ‡ã‚Šï¼‰
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
        sentence_count = len([s for s in sentences if s.strip()])

        # å¹³å‡æ–‡é•·
        avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0

        # é•·ã„æ–‡ã®æ•°ï¼ˆ20å˜èªä»¥ä¸Šï¼‰
        long_sentences = 0
        for sentence in sentences:
            if len(sentence.split()) > 20:
                long_sentences += 1

        # å¯èª­æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        readability_score = 100

        # é•·ã„æ–‡ãŒå¤šã„ã¨ãƒšãƒŠãƒ«ãƒ†ã‚£
        if sentence_count > 0:
            long_sentence_ratio = long_sentences / sentence_count
            readability_score -= long_sentence_ratio * 30

        # å¹³å‡æ–‡é•·ãŒé•·ã™ãã‚‹ã¨ãƒšãƒŠãƒ«ãƒ†ã‚£
        if avg_words_per_sentence > 15:
            readability_score -= (avg_words_per_sentence - 15) * 2

        readability_score = max(0, min(100, readability_score))

        return {
            "score": round(readability_score, 2),
            "sentences": sentence_count,
            "avg_words_per_sentence": round(avg_words_per_sentence, 2),
            "long_sentences": long_sentences,
            "readability_level": self._get_readability_level(readability_score)
        }

    def _get_readability_level(self, score: float) -> str:
        """å¯èª­æ€§ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        if score >= 90:
            return "éå¸¸ã«èª­ã¿ã‚„ã™ã„"
        elif score >= 80:
            return "èª­ã¿ã‚„ã™ã„"
        elif score >= 70:
            return "ã‚„ã‚„èª­ã¿ã‚„ã™ã„"
        elif score >= 60:
            return "æ™®é€š"
        elif score >= 50:
            return "ã‚„ã‚„èª­ã¿ã«ãã„"
        else:
            return "èª­ã¿ã«ãã„"

    def _calculate_structure_score(self, headers, links, images, code_blocks, tables, words) -> float:
        """æ§‹é€ å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0

        # è¦‹å‡ºã—æ§‹é€  (0-25ç‚¹)
        if headers["total"] > 0:
            score += min(headers["total"] * 5, 25)
            # éšå±¤å•é¡ŒãŒã‚ã‚Œã°ãƒšãƒŠãƒ«ãƒ†ã‚£
            if headers["hierarchy_issues"]:
                score -= len(headers["hierarchy_issues"]) * 3

        # ãƒªãƒ³ã‚¯ (0-20ç‚¹)
        if links["total"] > 0:
            score += min(links["total"] * 2, 20)
            # ç©ºãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°ãƒšãƒŠãƒ«ãƒ†ã‚£
            score -= links["empty"] * 2

        # è¦–è¦šè¦ç´  (0-20ç‚¹)
        if images["total"] > 0:
            score += min(images["total"] * 3, 15)
            # altãƒ†ã‚­ã‚¹ãƒˆãªã—ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
            score -= images["without_alt"] * 2

        if tables["total"] > 0:
            score += min(tables["total"] * 2, 5)

        # ã‚³ãƒ¼ãƒ‰ä¾‹ (0-15ç‚¹)
        if code_blocks["total_blocks"] > 0:
            score += min(code_blocks["total_blocks"] * 3, 15)

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ (0-20ç‚¹)
        if words > 0:
            score += min(words / 50, 20)

        return min(score, 100)

    def _simulate_ai_analysis(self, content: str, filename: str) -> Dict[str, Any]:
        """AIåˆ†æã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå°†æ¥çš„ã«ã¯GPT APIçµ±åˆï¼‰"""
        # ç°¡æ˜“AIåˆ†æã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        ai_score = 75  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢

        suggestions = []

        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        if len(content.split()) < 100:
            suggestions.append("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
            ai_score -= 10

        if not re.search(r'```', content):
            if 'api' in filename.lower() or 'code' in filename.lower():
                suggestions.append("æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚³ãƒ¼ãƒ‰ä¾‹ãŒã‚ã‚‹ã¨ç†è§£ã—ã‚„ã™ããªã‚Šã¾ã™ã€‚")
                ai_score -= 5

        if not re.search(r'!\[.*\]\(.*\)', content):
            if 'guide' in filename.lower() or 'tutorial' in filename.lower():
                suggestions.append("ã‚¬ã‚¤ãƒ‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯å›³è¡¨ãŒã‚ã‚‹ã¨åˆ†ã‹ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚")
                ai_score -= 5

        # è¦‹å‡ºã—æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if not re.search(r'^#+ ', content, re.MULTILINE):
            suggestions.append("é©åˆ‡ãªè¦‹å‡ºã—æ§‹é€ ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§æ–‡æ›¸ã®æ§‹é€ ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚")
            ai_score -= 15

        return {
            "score": max(0, ai_score),
            "suggestions": suggestions,
            "ai_enabled": self.ai_enabled,
            "analysis_method": "rule_based_simulation" if not self.ai_enabled else "gpt_analysis"
        }

    def _calculate_overall_score(self, structure_score: float, readability_score: float, ai_score: float) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # é‡ã¿ä»˜ãå¹³å‡
        weights = {
            "structure": 0.4,
            "readability": 0.3,
            "ai": 0.3
        }

        overall = (
            structure_score * weights["structure"] +
            readability_score * weights["readability"] +
            ai_score * weights["ai"]
        )

        return round(overall, 2)

    def _generate_quality_summary(self, content_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å“è³ªã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        scores = []
        for file_data in content_analysis.values():
            if "overall_score" in file_data:
                scores.append(file_data["overall_score"])

        avg_score = sum(scores) / len(scores) if scores else 0

        # ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        score_distribution = {
            "excellent": len([s for s in scores if s >= 90]),
            "good": len([s for s in scores if 80 <= s < 90]),
            "fair": len([s for s in scores if 70 <= s < 80]),
            "poor": len([s for s in scores if s < 70])
        }

        return {
            "average_score": round(avg_score, 2),
            "total_files": len(scores),
            "score_distribution": score_distribution,
            "quality_level": self._get_quality_level(avg_score)
        }

    def _get_quality_level(self, score: float) -> str:
        """å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        if score >= 90:
            return "å„ªç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "æ™®é€š"
        else:
            return "è¦æ”¹å–„"

    def _generate_ai_recommendations(self, content_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIæ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        # ä½ã‚¹ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        low_score_files = []
        for file_path, file_data in content_analysis.items():
            if "overall_score" in file_data and file_data["overall_score"] < 70:
                low_score_files.append((file_path, file_data))

        if low_score_files:
            recommendations.append({
                "priority": "high",
                "category": "content_quality",
                "title": "ä½å“è³ªãƒ•ã‚¡ã‚¤ãƒ«ã®æ”¹å–„",
                "description": f"{len(low_score_files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå“è³ªåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™",
                "action": "è¦‹å‡ºã—æ§‹é€ ã€ãƒªãƒ³ã‚¯ã€è¦–è¦šè¦ç´ ã®è¿½åŠ ãƒ»æ”¹å–„",
                "affected_files": [file for file, _ in low_score_files[:5]]  # æœ€åˆã®5ä»¶
            })

        # æ§‹é€ æ”¹å–„æ¨å¥¨
        structure_issues = 0
        for file_data in content_analysis.values():
            if "structure_analysis" in file_data:
                if file_data["structure_analysis"]["headers"]["hierarchy_issues"]:
                    structure_issues += 1

        if structure_issues > 0:
            recommendations.append({
                "priority": "medium",
                "category": "structure",
                "title": "è¦‹å‡ºã—éšå±¤ã®æ”¹å–„",
                "description": f"{structure_issues}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§è¦‹å‡ºã—éšå±¤ã«å•é¡ŒãŒã‚ã‚Šã¾ã™",
                "action": "é©åˆ‡ãªè¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ï¼ˆh1â†’h2â†’h3ï¼‰ã®ä½¿ç”¨",
                "impact": "æ–‡æ›¸æ§‹é€ ã®æ˜ç¢ºåŒ–ãƒ»ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š"
            })

        return recommendations

    def generate_detailed_report(self, analysis_data: Dict[str, Any]) -> str:
        """è©³ç´°HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        html_template = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIå“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ - WebSys</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 0; background: #f8fafc; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; text-align: center; }}
        .container {{ max-width: 1400px; margin: 0 auto; padding: 30px; }}
        .dashboard {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 25px; margin-bottom: 40px; }}
        .widget {{ background: white; border-radius: 16px; padding: 30px; box-shadow: 0 8px 32px rgba(0,0,0,0.1); transition: transform 0.2s; }}
        .widget:hover {{ transform: translateY(-2px); }}
        .widget h3 {{ margin-top: 0; color: #2d3748; font-size: 1.3em; }}
        .score-circle {{ width: 120px; height: 120px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin: 20px auto; font-size: 2em; font-weight: bold; }}
        .score-excellent {{ background: linear-gradient(135deg, #48bb78, #38a169); color: white; }}
        .score-good {{ background: linear-gradient(135deg, #4299e1, #3182ce); color: white; }}
        .score-fair {{ background: linear-gradient(135deg, #ed8936, #dd6b20); color: white; }}
        .score-poor {{ background: linear-gradient(135deg, #f56565, #e53e3e); color: white; }}
        .file-analysis {{ background: white; border-radius: 12px; padding: 25px; margin-bottom: 20px; box-shadow: 0 4px 16px rgba(0,0,0,0.08); }}
        .file-header {{ display: flex; justify-content: space-between; align-items: center; margin-bottom: 20px; }}
        .file-name {{ font-size: 1.2em; font-weight: 600; color: #2d3748; }}
        .metrics-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .metric {{ background: #f7fafc; padding: 15px; border-radius: 8px; text-align: center; }}
        .metric-value {{ font-size: 1.5em; font-weight: bold; color: #667eea; }}
        .metric-label {{ color: #718096; font-size: 0.9em; margin-top: 5px; }}
        .suggestions {{ margin-top: 20px; }}
        .suggestion {{ background: #ebf8ff; border-left: 4px solid #4299e1; padding: 15px; margin: 10px 0; border-radius: 0 8px 8px 0; }}
        .recommendations {{ background: white; border-radius: 16px; padding: 30px; margin-top: 30px; box-shadow: 0 8px 32px rgba(0,0,0,0.1); }}
        .recommendation {{ background: #f0fff4; border-left: 4px solid #48bb78; padding: 20px; margin: 15px 0; border-radius: 0 8px 8px 0; }}
        .recommendation h4 {{ margin: 0 0 10px 0; color: #2d3748; }}
        .chart-placeholder {{ background: #edf2f7; height: 200px; border-radius: 8px; display: flex; align-items: center; justify-content: center; color: #718096; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¤– AIå“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>WebSys Phase2 é«˜åº¦å“è³ªåˆ†æã‚·ã‚¹ãƒ†ãƒ </p>
        <p>ç”Ÿæˆæ—¥æ™‚: {self.timestamp.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
    </div>

    <div class="container">
        <div class="dashboard">
            <div class="widget">
                <h3>ğŸ“Š ç·åˆå“è³ªã‚¹ã‚³ã‚¢</h3>
                <div class="score-circle score-{self._get_score_class(analysis_data['quality_summary']['average_score'])}">
                    {analysis_data['quality_summary']['average_score']}
                </div>
                <div style="text-align: center; color: #718096;">
                    {analysis_data['quality_summary']['quality_level']}
                </div>
            </div>

            <div class="widget">
                <h3>ğŸ“š åˆ†æå¯¾è±¡</h3>
                <div class="metric-value">{analysis_data['quality_summary']['total_files']}</div>
                <div class="metric-label">ãƒ•ã‚¡ã‚¤ãƒ«</div>
            </div>

            <div class="widget">
                <h3>ğŸ† å„ªç§€ãƒ•ã‚¡ã‚¤ãƒ«</h3>
                <div class="metric-value">{analysis_data['quality_summary']['score_distribution']['excellent']}</div>
                <div class="metric-label">90ç‚¹ä»¥ä¸Š</div>
            </div>

            <div class="widget">
                <h3>âš ï¸ è¦æ”¹å–„ãƒ•ã‚¡ã‚¤ãƒ«</h3>
                <div class="metric-value">{analysis_data['quality_summary']['score_distribution']['poor']}</div>
                <div class="metric-label">70ç‚¹æœªæº€</div>
            </div>
        </div>

        <div class="recommendations">
            <h2>ğŸ’¡ AIæ¨å¥¨æ”¹å–„äº‹é …</h2>
        """

        for rec in analysis_data['ai_recommendations']:
            html_template += f"""
            <div class="recommendation">
                <h4>{rec['title']}</h4>
                <p><strong>èª¬æ˜:</strong> {rec['description']}</p>
                <p><strong>æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:</strong> {rec['action']}</p>
            </div>
            """

        html_template += """
        </div>

        <h2>ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è©³ç´°åˆ†æ</h2>
        """

        # ä¸Šä½ãƒ»ä¸‹ä½ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤ºï¼ˆè¡¨ç¤ºé‡åˆ¶é™ï¼‰
        sorted_files = sorted(
            analysis_data['content_analysis'].items(),
            key=lambda x: x[1].get('overall_score', 0),
            reverse=True
        )

        for file_path, file_data in sorted_files[:10]:  # ä¸Šä½10ä»¶
            if 'overall_score' in file_data:
                score_class = self._get_score_class(file_data['overall_score'])
                html_template += f"""
                <div class="file-analysis">
                    <div class="file-header">
                        <div class="file-name">{Path(file_path).name}</div>
                        <div class="score-circle score-{score_class}" style="width: 60px; height: 60px; font-size: 1.2em;">
                            {file_data['overall_score']}
                        </div>
                    </div>

                    <div class="metrics-grid">
                        <div class="metric">
                            <div class="metric-value">{file_data['basic_metrics']['words']}</div>
                            <div class="metric-label">å˜èªæ•°</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{file_data['structure_analysis']['headers']['total']}</div>
                            <div class="metric-label">è¦‹å‡ºã—æ•°</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{file_data['structure_analysis']['links']['total']}</div>
                            <div class="metric-label">ãƒªãƒ³ã‚¯æ•°</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{file_data['readability']['readability_level']}</div>
                            <div class="metric-label">å¯èª­æ€§</div>
                        </div>
                    </div>

                    {f'<div class="suggestions"><h4>AIæ”¹å–„ææ¡ˆ:</h4>' + ''.join([f'<div class="suggestion">{suggestion}</div>' for suggestion in file_data['ai_analysis']['suggestions']]) + '</div>' if file_data['ai_analysis']['suggestions'] else ''}
                </div>
                """

        html_template += """
    </div>
</body>
</html>
        """

        return html_template

    def _get_score_class(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã«åŸºã¥ãCSSã‚¯ãƒ©ã‚¹å–å¾—"""
        if score >= 90:
            return "excellent"
        elif score >= 80:
            return "good"
        elif score >= 70:
            return "fair"
        else:
            return "poor"

def main():
    parser = argparse.ArgumentParser(description='WebSys AI Quality Analyzer')
    parser.add_argument('--docs-dir', default='docs', help='ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--output-dir', default='docs/quality-reports', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--format', choices=['json', 'html', 'both'], default='both', help='å‡ºåŠ›å½¢å¼')
    parser.add_argument('--ai-enabled', action='store_true', help='å®Ÿéš›ã®AIåˆ†æã‚’æœ‰åŠ¹åŒ–')

    args = parser.parse_args()

    print("ğŸ¤– WebSys AIå“è³ªåˆ†æã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.docs_dir}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
    print(f"AIåˆ†æ: {'æœ‰åŠ¹' if args.ai_enabled else 'ç„¡åŠ¹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰'}")
    print()

    analyzer = AIQualityAnalyzer(args.docs_dir, args.output_dir)
    analyzer.ai_enabled = args.ai_enabled

    analysis_data = analyzer.analyze_content_quality()

    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')

    # JSONå‡ºåŠ›
    if args.format in ['json', 'both']:
        json_file = Path(args.output_dir) / f"ai-quality-{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… AIåˆ†æJSONå‡ºåŠ›: {json_file}")

    # HTMLå‡ºåŠ›
    if args.format in ['html', 'both']:
        html_content = analyzer.generate_detailed_report(analysis_data)
        html_file = Path(args.output_dir) / f"ai-quality-{timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"âœ… AIåˆ†æHTMLå‡ºåŠ›: {html_file}")

    print("\nğŸ‰ AIå“è³ªåˆ†æå®Œäº†ï¼")
    print(f"ğŸ¤– ç·åˆã‚¹ã‚³ã‚¢: {analysis_data['quality_summary']['average_score']}/100")
    print(f"ğŸ“š åˆ†æãƒ•ã‚¡ã‚¤ãƒ«: {analysis_data['quality_summary']['total_files']}ä»¶")
    print(f"ğŸ† å“è³ªãƒ¬ãƒ™ãƒ«: {analysis_data['quality_summary']['quality_level']}")
    print(f"ğŸ’¡ æ”¹å–„ææ¡ˆ: {len(analysis_data['ai_recommendations'])}é …ç›®")

if __name__ == "__main__":
    main()